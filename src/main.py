import os
import logging
import argparse
import sys
from datetime import datetime

from scraper import PrefeituraScraper
from uploader import FileUploader
from database import DatabaseManager

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("natal_scraping.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_full_process(headless=True):
    start_time = datetime.now()
    logger.info(f"ğŸš€ Iniciando processo completo Ã s {start_time}")
    
    try:
        logger.info("ğŸ¤– Iniciando scraping do site da prefeitura...")
        scraper = PrefeituraScraper(headless=headless)
        publications = scraper.run()
        
        if not publications:
            logger.warning("âš ï¸ Nenhuma publicaÃ§Ã£o encontrada. O processo serÃ¡ encerrado com sucesso, mas sem novos dados.")
            return True
        
        logger.info(f"âœ… Scraping concluÃ­do. {len(publications)} publicaÃ§Ãµes encontradas.")

        logger.info("â˜ï¸ Iniciando upload dos arquivos...")
        uploader = FileUploader()
        uploaded_publications = uploader.upload_multiple_files(publications)
        
        if not uploaded_publications:
            logger.error("âŒ Nenhum arquivo foi enviado com sucesso. Encerrando com falha.")
            return False
        
        logger.info(f"âœ… Upload concluÃ­do. {len(uploaded_publications)} arquivos enviados.")

        logger.info("ğŸ’¾ Iniciando armazenamento no banco de dados...")
        db_manager = DatabaseManager()
        saved_count = db_manager.save_publications(uploaded_publications)
        
        logger.info(f"âœ… Armazenamento concluÃ­do. {saved_count} novas publicaÃ§Ãµes salvas.")

        end_time = datetime.now()
        duration = end_time - start_time
        logger.info(f"ğŸ‰ Processo concluÃ­do com sucesso em {duration}")
        
        return True
    
    except Exception as e:
        logger.error(f"âŒ Erro fatal durante a execuÃ§Ã£o do processo: {str(e)}", exc_info=True)
        sys.exit(1) # Garante que o contÃªiner Docker pare com um cÃ³digo de erro

def run_api_only():
    from api import app
    import uvicorn
    
    logger.info("ğŸš€ Iniciando apenas a API...")
    uvicorn.run(app, host="0.0.0.0", port=8000)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Scraping de publicaÃ§Ãµes da Prefeitura de Natal")
    parser.add_argument("--api-only", action="store_true", help="Executa apenas a API")
    parser.add_argument("--no-headless", action="store_true", help="Executa o navegador em modo visÃ­vel para depuraÃ§Ã£o")
    
    args = parser.parse_args()
    
    if args.api_only:
        run_api_only()
    else:
        success = run_full_process(headless=not args.no_headless)
        if not success:
             logger.error("O processo de scraping falhou. Verifique os logs.")
             sys.exit(1)